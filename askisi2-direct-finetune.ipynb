{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.optim import AdamW\nfrom torch.nn import MSELoss\n\n# Load the dataset\ndataset = load_dataset(\"nvidia/HelpSteer\")\ntrain_df = pd.DataFrame(dataset['train'])\nvalidation_df = pd.DataFrame(dataset['validation'])\n\n# Set up the device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load DistilBERT tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Load DistilBERT model with a regression head\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    'distilbert-base-uncased',\n    num_labels=1  # Single output for regression\n).to(device)\n\n# Prepare the dataset\nclass HelpSteerDataset(Dataset):\n    def __init__(self, prompts, responses, labels, tokenizer, max_length=128):\n        self.prompts = prompts\n        self.responses = responses\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Concatenate prompt and response with a separator token\n        text = self.prompts[idx] + \" [SEP] \" + self.responses[idx]\n        encoding = self.tokenizer(text, \n                                  max_length=self.max_length, \n                                  truncation=True, \n                                  padding=\"max_length\", \n                                  return_tensors=\"pt\")\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n        }\n\n# Split the data into features (X) and target (y)\nX = train_df[['prompt', 'response']].values  # Prompts and responses\ny = train_df['complexity'].values  # Target variable (complexity)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create train and validation datasets\ntrain_dataset = HelpSteerDataset(\n    prompts=[x[0] for x in X_train],\n    responses=[x[1] for x in X_train],\n    labels=y_train,\n    tokenizer=tokenizer\n)\n\ntest_dataset = HelpSteerDataset(\n    prompts=[x[0] for x in X_test],\n    responses=[x[1] for x in X_test],\n    labels=y_test,\n    tokenizer=tokenizer\n)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Set up the optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=2e-5)\ncriterion = MSELoss()  # Mean Squared Error for regression\n\n# Training loop\nepochs = 3\nmodel.train()\n\nfor epoch in range(epochs):\n    total_loss = 0\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    \n    for batch in tqdm(train_loader, desc=\"Training\"):\n        optimizer.zero_grad()\n\n        # Move data to GPU\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels.unsqueeze(1))\n        loss = outputs.loss\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}\")\n\n# Evaluate the model\nmodel.eval()\nall_predictions = []\nall_targets = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n        # Move data to GPU\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = outputs.logits.squeeze()\n\n        # Store predictions and targets\n        all_predictions.extend(predictions.cpu().numpy())\n        all_targets.extend(labels.cpu().numpy())\n\n# Compute evaluation metrics\nrmse = mean_squared_error(all_targets, all_predictions, squared=False)\nmae = mean_absolute_error(all_targets, all_predictions)\n\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"MAE: {mae:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T17:47:00.818755Z","iopub.execute_input":"2024-11-19T17:47:00.819413Z","iopub.status.idle":"2024-11-19T18:10:13.504279Z","shell.execute_reply.started":"2024-11-19T17:47:00.819381Z","shell.execute_reply":"2024-11-19T18:10:13.503361Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 884/884 [07:19<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Average Loss: 0.6383\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 884/884 [07:13<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Average Loss: 0.4363\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 884/884 [07:13<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Average Loss: 0.2931\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 221/221 [01:22<00:00,  2.68it/s]","output_type":"stream"},{"name":"stdout","text":"RMSE: 0.5992\nMAE: 0.4519\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34}]}